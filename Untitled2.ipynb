{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import os, glob, cv2\n",
    "from scipy.ndimage import imread\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_vgg16_conv = VGG16(weights='imagenet', include_top=False)\n",
    "model_vgg16_conv.summary()\n",
    "for layer in model_vgg16_conv.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mens\n",
      "womens\n"
     ]
    }
   ],
   "source": [
    "directory_parent = '/Users/jeff/Desktop/CS156/5.1/'\n",
    "folders = ['mens', 'womens']\n",
    "\n",
    "data_X = []\n",
    "data_y = []\n",
    "\n",
    "for i in range(len(folders)):\n",
    "    print folders[i]\n",
    "    folder_name = folders[i]\n",
    "    directory = directory_parent + folder_name + '/'\n",
    "    images_arr = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if 'resized_224' in filename:\n",
    "            img = image.load_img(directory + filename)\n",
    "            x = image.img_to_array(img)\n",
    "            x = preprocess_input(x)\n",
    "            data_X.append(x)\n",
    "            y = np.zeros(2)\n",
    "            y[i] = 1\n",
    "            data_y.append(y)\n",
    "            \n",
    "data_X = np.stack(data_X, axis=0)\n",
    "data_y = np.stack(data_y, axis=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"pr..., inputs=Tensor(\"im...)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 10)                250890    \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 14,965,600\n",
      "Trainable params: 250,912\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "2009/2009 [==============================] - 1114s 555ms/step - loss: 7.0312\n",
      "Epoch 2/15\n",
      "2009/2009 [==============================] - 1112s 554ms/step - loss: 0.6439\n",
      "Epoch 3/15\n",
      "2009/2009 [==============================] - 1180s 587ms/step - loss: 0.4357\n",
      "Epoch 4/15\n",
      "2009/2009 [==============================] - 1209s 602ms/step - loss: 0.2926\n",
      "Epoch 5/15\n",
      "2009/2009 [==============================] - 1103s 549ms/step - loss: 0.1749\n",
      "Epoch 6/15\n",
      "2009/2009 [==============================] - 973s 484ms/step - loss: 0.1496\n",
      "Epoch 7/15\n",
      "2009/2009 [==============================] - 947s 471ms/step - loss: 0.1174\n",
      "Epoch 8/15\n",
      "2009/2009 [==============================] - 928s 462ms/step - loss: 0.0899\n",
      "Epoch 9/15\n",
      "2009/2009 [==============================] - 956s 476ms/step - loss: 0.1110\n",
      "Epoch 10/15\n",
      "2009/2009 [==============================] - 942s 469ms/step - loss: 0.0946\n",
      "Epoch 11/15\n",
      "2009/2009 [==============================] - 952s 474ms/step - loss: 0.0803\n",
      "Epoch 12/15\n",
      "2009/2009 [==============================] - 972s 484ms/step - loss: 0.0643\n",
      "Epoch 13/15\n",
      "2009/2009 [==============================] - 952s 474ms/step - loss: 0.0552\n",
      "Epoch 14/15\n",
      "2009/2009 [==============================] - 929s 462ms/step - loss: 0.0498\n",
      "Epoch 15/15\n",
      "2009/2009 [==============================] - 922s 459ms/step - loss: 0.0442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11aa328d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = Input(shape=(224,224,3),name = 'image_input')\n",
    "output_vgg16_conv = model_vgg16_conv(input)\n",
    "\n",
    "#Add the fully-connected layers \n",
    "x = Flatten(name='flatten')(output_vgg16_conv)\n",
    "x = Dense(10, activation='relu', name='fc')(x)\n",
    "x = Dense(2, activation='softmax', name='predictions')(x)\n",
    "model = Model(input=input, output=x)\n",
    "\n",
    "#In the summary, weights and layers from VGG part will be hidden, but they will be fit during the training\n",
    "model.summary()\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('mens-womens-fashion-classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.93159007e-06,   9.99996066e-01],\n",
       "       [  1.00000000e+00,   6.59215749e-09],\n",
       "       [  9.99895930e-01,   1.04031627e-04],\n",
       "       [  1.00000000e+00,   1.81292901e-08],\n",
       "       [  4.06399876e-13,   1.00000000e+00],\n",
       "       [  1.00000000e+00,   1.23902458e-16],\n",
       "       [  1.00000000e+00,   1.14133800e-13],\n",
       "       [  2.21414900e-12,   1.00000000e+00],\n",
       "       [  9.23657136e-16,   1.00000000e+00],\n",
       "       [  3.37916463e-05,   9.99966264e-01]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manual_test_files = ['suit.png', 'women-cashmere.png']\n",
    "manual_test_X = []\n",
    "for filename in manual_test_files:\n",
    "    img = image.load_img(filename, target_size=(224,224,3))\n",
    "    x = image.img_to_array(img)\n",
    "    x = preprocess_input(x)\n",
    "    manual_test_X.append(x)\n",
    "manual_test_X = np.stack(manual_test_X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   6.35195089e-16],\n",
       "       [  6.80351106e-04,   9.99319673e-01]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(manual_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
